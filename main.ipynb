{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "struct Constant{T} <: GraphNode\n",
    "    output :: T\n",
    "end\n",
    "\n",
    "mutable struct Variable <: GraphNode\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name::String\n",
    "    batch_gradient::Any\n",
    "    Variable(output; name = \"?\") = new(output, nothing, name, nothing)\n",
    "end\n",
    "\n",
    "mutable struct ScalarOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct BroadcastedOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    cache :: Any\n",
    "    BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name, nothing)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topological_sort (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function visit(node::GraphNode, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "    \n",
    "function visit(node::Operator, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function topological_sort(head::GraphNode)\n",
    "    visited = Set()\n",
    "    order = Vector()\n",
    "    visit(head, visited, order)\n",
    "    return order\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forward! (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset!(node::Constant) = nothing\n",
    "reset!(node::Variable) = node.gradient = nothing\n",
    "reset!(node::Operator) = node.gradient = nothing\n",
    "\n",
    "compute!(node::Constant) = nothing\n",
    "compute!(node::Variable) = nothing\n",
    "compute!(node::Operator) =\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "\n",
    "function forward!(order::Vector)\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        reset!(node)\n",
    "    end\n",
    "    return last(order).output\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "update!(node::Constant, gradient) = nothing\n",
    "update!(node::GraphNode, gradient) = let\n",
    "    node.gradient = gradient\n",
    "    if typeof(node) == Variable\n",
    "        if isnothing(node.batch_gradient)\n",
    "            node.batch_gradient = gradient\n",
    "        else\n",
    "            node.batch_gradient .+= gradient\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "function backward!(order::Vector; seed=1.0)\n",
    "    result = last(order)\n",
    "    result.gradient = seed\n",
    "    @assert length(result.output) == 1 \"Gradient is defined only for scalar functions\"\n",
    "    for node in reverse(order)\n",
    "        backward!(node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function backward!(node::Constant) end\n",
    "function backward!(node::Variable) end\n",
    "function backward!(node::Operator)\n",
    "    inputs = node.inputs\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 4 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import Base: sum\n",
    "\n",
    "rnnCell(U :: GraphNode, W :: GraphNode, h :: GraphNode, b :: GraphNode, x :: GraphNode) = BroadcastedOperator(rnnCell, U, W, h, b, x)\n",
    "forward(::BroadcastedOperator{typeof(rnnCell)}, U, W, h, b, x) = let\n",
    "    Uh_mul = U * x\n",
    "    Wx_mul = W * h\n",
    "\n",
    "    vectors_sum = Uh_mul + Wx_mul + b\n",
    "     \n",
    "    return tanh.(vectors_sum)\n",
    "end\n",
    "backward(::BroadcastedOperator{typeof(rnnCell)}, U, W, h, b, x, g) = let \n",
    "    Uh_mul = U * x\n",
    "    Wx_mul = W * h\n",
    "    vectors_sum = Uh_mul + Wx_mul + b\n",
    "\n",
    "    dh = g .* (1 .- tanh.(vectors_sum) .^ 2)\n",
    "\n",
    "    dU = dh * x'\n",
    "    dW = dh * h'\n",
    "    db = sum(dh, dims=2)\n",
    "    dx = U' * dh\n",
    "    dh_prev = W' * dh\n",
    "\n",
    "    return tuple(dU, dW, dh_prev, db, dx)\n",
    "end\n",
    "\n",
    "\n",
    "dense(x::GraphNode, w::GraphNode) = BroadcastedOperator(dense, x, w)\n",
    "forward(::BroadcastedOperator{typeof(dense)}, x, w) = w * x\n",
    "backward(::BroadcastedOperator{typeof(dense)}, x, w, g) = tuple(w' * g, g * x', g)\n",
    "\n",
    "identity(x::GraphNode) = BroadcastedOperator(identity, x)\n",
    "forward(::BroadcastedOperator{typeof(identity)}, x) = x\n",
    "backward(::BroadcastedOperator{typeof(identity)}, x, g) = tuple(g)\n",
    "\n",
    "cross_entropy_loss(y_hat::GraphNode, y::GraphNode) = BroadcastedOperator(cross_entropy_loss, y_hat, y)\n",
    "forward(::BroadcastedOperator{typeof(cross_entropy_loss)}, y_hat, y) = let\n",
    "    global predictions\n",
    "    global correct_predictions\n",
    "\n",
    "    predictions += 1\n",
    "    if argmax(y_hat) == argmax(y)\n",
    "        correct_predictions += 1\n",
    "    end\n",
    "    \n",
    "    y_hat = y_hat .- maximum(y_hat)\n",
    "    y_hat = exp.(y_hat) ./ sum(exp.(y_hat))\n",
    "    loss = sum(log.(y_hat) .* y) * -1.0\n",
    "    return loss\n",
    "end\n",
    "backward(::BroadcastedOperator{typeof(cross_entropy_loss)}, y_hat, y, g) = let\n",
    "    y_hat = y_hat .- maximum(y_hat)\n",
    "    y_hat = exp.(y_hat) ./ sum(exp.(y_hat))\n",
    "    return tuple(g .* (y_hat .- y))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Random\n",
    "using Printf\n",
    "predictions = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "mutable struct myRNN\n",
    "    WW\n",
    "    WU\n",
    "    WV\n",
    "    bh\n",
    "    by\n",
    "    h\n",
    "end\n",
    "\n",
    "function update_weights!(graph::Vector, lr::Float64, batch_size::Int64)\n",
    "    for node in graph\n",
    "        if isa(node, Variable) && hasproperty(node, :batch_gradient)\n",
    "\t\t\tnode.batch_gradient ./= batch_size\n",
    "            node.output .-= lr * node.batch_gradient \n",
    "            fill(node.batch_gradient, 0)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function build_graph(x, y, rnn::myRNN, j:: Number)\n",
    "    l1 = rnnCell(rnn.WU, rnn.WW, rnn.h, rnn.bh, Constant(x[1:196, j]))\n",
    "    l2 = rnnCell(rnn.WU, rnn.WW, l1, rnn.bh, Constant(x[197:392, j]))\n",
    "    l3 = rnnCell(rnn.WU, rnn.WW, l2, rnn.bh, Constant(x[393:588, j]))\n",
    "    l4 = rnnCell(rnn.WU, rnn.WW, l3, rnn.bh, Constant(x[589:end, j]))\n",
    "    l5 = dense(l4, rnn.WV) |> identity\n",
    "    e = cross_entropy_loss(l5, y)\n",
    "\n",
    "    return topological_sort(e)\n",
    "end\n",
    "\n",
    "\n",
    "function train(rnn::myRNN, x::Any, y::Any, epochs, batch_size, learning_rate)\n",
    "\n",
    "    for i=1:epochs\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        samples = size(x, 2)\n",
    "\n",
    "        global correct_predictions = 0\n",
    "        global predictions = 0\n",
    "\n",
    "        @time for j=1:samples\n",
    "            y_train = Constant(y[:, j])\n",
    "            \n",
    "            graph = build_graph(x, y_train, rnn, j)\n",
    "            rnn.h = Variable(zeros(64))\n",
    "            epoch_loss += forward!(graph)\n",
    "            backward!(graph)\n",
    "\n",
    "            if j % batch_size == 0\n",
    "                update_weights!(graph, learning_rate, batch_size)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        epoch = \"Epoch $i\"\n",
    "        loss = epoch_loss/samples\n",
    "        acc_calc = round(100 * (correct_predictions/predictions), digits=2)\n",
    "        train_acc = \"$acc_calc %\"\n",
    "\n",
    "        @info epoch loss train_acc\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "function test(rnn::myRNN, x::Any, y::Any)\n",
    "\n",
    "    samples = size(x, 2)\n",
    "\n",
    "    global correct_predictions = 0\n",
    "    global predictions = 0\n",
    "\n",
    "    @time for j=1:samples\n",
    "        y_train = Constant(y[:, j])\n",
    "        graph = build_graph(x, y_train, rnn, j)\n",
    "        rnn.h = Variable(zeros(64))\n",
    "        forward!(graph)\n",
    "    end\n",
    "\n",
    "    test = \"Test\"\n",
    "    acc_calc = round(100 * (correct_predictions/predictions), digits=2)\n",
    "    test_acc = \"$acc_calc %\"\n",
    "\n",
    "    @info test test_acc\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings and launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n",
      " 14.473325 seconds (40.37 M allocations: 33.483 GiB, 17.30% gc time, 11.90% compilation time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 1\n",
      "│   loss = 0.9640906713744877\n",
      "│   train_acc = 74.1 %\n",
      "└ @ Main /Users/narodzi/Desktop/awid-projekt/main.ipynb:66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12.008625 seconds (36.07 M allocations: 33.196 GiB, 16.81% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 2\n",
      "│   loss = 0.4704312321099117\n",
      "│   train_acc = 87.41 %\n",
      "└ @ Main /Users/narodzi/Desktop/awid-projekt/main.ipynb:66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12.001599 seconds (36.07 M allocations: 33.196 GiB, 17.12% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 3\n",
      "│   loss = 0.37615601475070337\n",
      "│   train_acc = 89.71 %\n",
      "└ @ Main /Users/narodzi/Desktop/awid-projekt/main.ipynb:66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 12.011165 seconds (36.07 M allocations: 33.196 GiB, 17.00% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 4\n",
      "│   loss = 0.32743266330230286\n",
      "│   train_acc = 90.88 %\n",
      "└ @ Main /Users/narodzi/Desktop/awid-projekt/main.ipynb:66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 11.922957 seconds (36.07 M allocations: 33.196 GiB, 16.87% gc time)\n",
      "Testing model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 5\n",
      "│   loss = 0.29511370468676784\n",
      "│   train_acc = 91.74 %\n",
      "└ @ Main /Users/narodzi/Desktop/awid-projekt/main.ipynb:66\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.292641 seconds (2.03 M allocations: 194.522 MiB, 4.88% gc time)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Test\n",
      "│   test_acc = 92.2 %\n",
      "└ @ Main /Users/narodzi/Desktop/awid-projekt/main.ipynb:89\n"
     ]
    }
   ],
   "source": [
    "using MLDatasets: MNIST\n",
    "using Flux\n",
    "train_data = MNIST(split=:train)  \n",
    "test_data  = MNIST(split=:test)\n",
    "\n",
    "x_train = reshape(train_data.features, 28 * 28, :)\n",
    "y_train  = Flux.onehotbatch(train_data.targets, 0:9)\n",
    "\n",
    "x_test = reshape(test_data.features, 28 * 28, :)\n",
    "y_test  = Flux.onehotbatch(test_data.targets, 0:9)\n",
    "\n",
    "WW = Variable(Flux.glorot_uniform(64,64))\n",
    "WU = Variable(Flux.glorot_uniform(64,14*14))\n",
    "WV = Variable(Flux.glorot_uniform(10,64))\n",
    "bh = Variable(zeros(64))\n",
    "by = Variable(zeros(10))\n",
    "h = Variable(zeros(64))\n",
    "\n",
    "rnn = myRNN(WW, WU, WV, bh, by, h)\n",
    "\n",
    "settings = (;\n",
    "    eta = 15e-3,\n",
    "    epochs = 5,\n",
    "    batch_size = 100,\n",
    ")\n",
    "\n",
    "println(\"Training model...\")\n",
    "train(rnn, x_train, y_train, settings.epochs, settings.batch_size, settings.eta)\n",
    "\n",
    "println(\"Testing model...\")\n",
    "test(rnn, x_test, y_test) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.3",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
